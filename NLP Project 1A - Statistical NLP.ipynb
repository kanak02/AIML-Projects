{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c0290c3-dfe1-495d-b988-959d346ec8f6",
   "metadata": {},
   "source": [
    "# Statistical NLP Part A"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3105e955-9c09-46d5-bc23-6273419b6eff",
   "metadata": {},
   "source": [
    "• DOMAIN: Digital content management\n",
    "• CONTEXT: Classification is probably the most popular task that you would deal with in real life. Text in the form of blogs, posts, articles, etc.\n",
    "are written every second. It is a challenge to predict the information about the writer without knowing about him/her. We are going to create a\n",
    "classifier that predicts multiple features of the author of a given text. We have designed it as a Multi label classification problem.\n",
    "• DATA DESCRIPTION: Over 600,000 posts from more than 19 thousand bloggers The Blog Authorship Corpus consists of the collected posts of\n",
    "19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or\n",
    "approximately 35 posts and 7250 words per person. Each blog is presented as a separate file, the name of which indicates a blogger id# and\n",
    "the blogger’s self-provided gender, age, industry, and astrological sign. (All are labelled for gender and age but for many, industry and/or sign is\n",
    "marked as unknown.) All bloggers included in the corpus fall into one of three age groups:\n",
    "• 8240 \"10s\" blogs (ages 13-17),\n",
    "• 8086 \"20s\" blogs(ages 23-27) and\n",
    "• 2994 \"30s\" blogs (ages 33-47)\n",
    "• For each age group, there is an equal number of male and female bloggers. Each blog in the corpus includes at least 200 occurrences of\n",
    "common English words. All formatting has been stripped with two exceptions. Individual posts within a single blogger are separated by the\n",
    "date of the following post and links within a post are denoted by the label url link.\n",
    "• PROJECT OBJECTIVE: To build a NLP classifier which can use input text parameters to determine the label/s of the blog. Specific to this case\n",
    "study, you can consider the text of the blog: ‘text’ feature as independent variable and ‘topic’ as dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8e8428-d5df-4370-888d-86ab1f1dcbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blogtext.csv']\n",
      "id,gender,age,topic,sign,date,text\n",
      "2059027,male,15,Student,Leo,\"14,May,2004\",\"           Info has been found (+/- 100 pages, and 4.5 MB of .pdf files) Now i have to wait untill our team leader has processed it and learns html.         \"\n",
      "2059027,male,15,Student,Leo,\"13,May,2004\",\"           These are the team members:   Drewes van der Laag           urlLink mail  Ruiyu Xie                     urlLink mail  Bryan Aaldering (me)          urlLink mail          \"\n",
      "2059027,male,15,Student,Leo,\"12,May,2004\",\"           In het kader van kernfusie op aarde:  MAAK JE EIGEN WATERSTOFBOM   How to build an H-Bomb From: ascott@tartarus.uwa.edu.au (Andrew Scott) Newsgroups: rec.humor Subject: How To Build An H-Bomb (humorous!) Date: 7 Feb 1994 07:41:14 GMT Organization: The University of Western Australia  Original file dated 12th November 1990. Seemed to be a transcript of a 'Seven Days' article. Poorly formatted and corrupted. I have added the text between 'examine under a microscope' and 'malleable\n"
     ]
    }
   ],
   "source": [
    "#1. Read and Analyse Dataset.\n",
    "\n",
    "# Load required libraries\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Extracting the zip file\n",
    "with zipfile.ZipFile('blogtext.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('blogs_data')\n",
    "\n",
    "# Listing the extracted files\n",
    "extracted_files = os.listdir('blogs_data')\n",
    "print(extracted_files)\n",
    "\n",
    "# Assuming the data is in a CSV or text format, let's load the first file as a sample to inspect\n",
    "file_path = 'blogs_data/' + extracted_files[0]  # Adjust this based on the actual file format\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    sample_data = file.read()\n",
    "\n",
    "# Print sample data to inspect\n",
    "print(sample_data[:1000])  # Print the first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51561298-fdab-4246-8852-4fcf18b681d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4507324f-6209-47e4-b3d3-e7138c2f12af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in Dataset:\n",
      " id        0\n",
      "gender    0\n",
      "age       0\n",
      "topic     0\n",
      "sign      0\n",
      "date      0\n",
      "text      0\n",
      "dtype: int64\n",
      "Statistics on Blog Text Length:\n",
      " count    681284.000000\n",
      "mean       1120.730698\n",
      "std        2328.437003\n",
      "min           4.000000\n",
      "25%         230.000000\n",
      "50%         637.000000\n",
      "75%        1407.000000\n",
      "max      790123.000000\n",
      "Name: text_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# #1A. Clearly write outcome of data analysis\n",
    "# #1B. Clean the Structured Data\n",
    "# #1B. i. Missing value analysis and imputation.\n",
    "\n",
    "# Checking for missing values again after handling the extraction\n",
    "print(\"Missing Values in Dataset:\\n\", df.isnull().sum())\n",
    "\n",
    "# Basic statistics of the text data (e.g., average length of blog posts)\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "print(\"Statistics on Blog Text Length:\\n\", df['text_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c1096b-1ab7-493d-be95-9c558358698c",
   "metadata": {},
   "source": [
    "### Note to the evaluator:\r\n",
    "My system's configuration is insufficient to process the full dataset, so I have limited the data to 50,000 rows.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cd4fd53-886d-480c-abef-9f9506ad1f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe limited to 50000 rows\n",
      "Statistics on Blog Text Length:\n",
      " count     50000.000000\n",
      "mean       1130.585300\n",
      "std        2216.412948\n",
      "min           4.000000\n",
      "25%         237.000000\n",
      "50%         662.000000\n",
      "75%        1460.000000\n",
      "max      321278.000000\n",
      "Name: text_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Limiting the dataframe to 50,000 rows\n",
    "df = df.head(50000)\n",
    "print(f\"Dataframe limited to {df.shape[0]} rows\")\n",
    "\n",
    "# Basic statistics of the text data (e.g., average length of blog posts)\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "print(\"Statistics on Blog Text Length:\\n\", df['text_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4156470c-4546-460c-946f-1496ec9d1b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining rows after removing non-English posts: 47746\n"
     ]
    }
   ],
   "source": [
    "#1B. ii. Eliminate Non-English textual data.\n",
    "\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "# Function to detect language\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Apply the function to detect language of each blog post\n",
    "df['language'] = df['text'].apply(detect_language)\n",
    "\n",
    "# Filter to keep only English blogs\n",
    "df = df[df['language'] == 'en']\n",
    "\n",
    "# Checking the remaining dataset after filtering\n",
    "print(\"Remaining rows after removing non-English posts:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c457908-04ab-4cc5-846b-f4149de559db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0             Info has been found (+/- 100 pages,...   \n",
      "2             In het kader van kernfusie op aarde...   \n",
      "3                   testing!!!  testing!!!             \n",
      "4               Thanks to Yahoo!'s Toolbar I can ...   \n",
      "5               I had an interesting conversation...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0             Info has been found   pages and  MB...  \n",
      "2             In het kader van kernfusie op aarde...  \n",
      "3                         testing  testing            \n",
      "4               Thanks to Yahoos Toolbar I can no...  \n",
      "5               I had an interesting conversation...  \n"
     ]
    }
   ],
   "source": [
    "#2. Preprocess unstructured data to make it consumable for model training.\n",
    "#2A. Eliminate All special Characters and Numbers\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Keep only letters and spaces\n",
    "    return text\n",
    "\n",
    "# Apply cleaning function to text column\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Preview cleaned data\n",
    "print(df[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d64dab8-0503-41db-85d6-79db8aae30d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text\n",
      "0             info has been found   pages and  mb...\n",
      "2             in het kader van kernfusie op aarde...\n",
      "3                         testing  testing          \n",
      "4               thanks to yahoos toolbar i can no...\n",
      "5               i had an interesting conversation...\n"
     ]
    }
   ],
   "source": [
    "#2B. Lowercase all textual data\n",
    "\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Preview cleaned data\n",
    "print(df[['cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b69d234-227d-425d-ab1a-1de249f208e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kanak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text\n",
      "0  info found pages mb pdf files wait untill team...\n",
      "2  het kader van kernfusie op aarde maak je eigen...\n",
      "3                                    testing testing\n",
      "4  thanks yahoos toolbar capture urls popupswhich...\n",
      "5  interesting conversation dad morning talking k...\n"
     ]
    }
   ],
   "source": [
    "#2C. Remove all Stopwords\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove Stopwords\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Preview cleaned data\n",
    "print(df[['cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cfc3d0b-d47b-4b98-80bf-a9d533b3072a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text\n",
      "0  info found pages mb pdf files wait untill team...\n",
      "2  het kader van kernfusie op aarde maak je eigen...\n",
      "3                                    testing testing\n",
      "4  thanks yahoos toolbar capture urls popupswhich...\n",
      "5  interesting conversation dad morning talking k...\n"
     ]
    }
   ],
   "source": [
    "#2D. Remove all extra white spaces\n",
    "\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "\n",
    "# Preview final cleaned data\n",
    "print(df[['cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c549cb50-f692-4d38-b51e-22237863b623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (47746,)\n",
      "Shape of y: (47746,)\n"
     ]
    }
   ],
   "source": [
    "#3. Build a base Classification model\n",
    "#3A. Create dependent and independent variables\n",
    "\n",
    "X = df['cleaned_text']  # Independent variable (text)\n",
    "y = df['gender']        # Dependent variable (target)\n",
    "\n",
    "# Checking the shapes\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1095f3b-9f47-4f12-9d20-4ec22fed4635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (38196,)\n",
      "Testing set shape: (9550,)\n"
     ]
    }
   ],
   "source": [
    "#3B. Split data into train and test.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15565b69-da28-482e-9ff2-b5776d508c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF training data: (38196, 5000)\n",
      "Shape of TF-IDF testing data: (9550, 5000)\n"
     ]
    }
   ],
   "source": [
    "#3C. Vectorize data using any one vectorizer.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit the number of features to 5000 for faster processing\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Check the shape of the vectorized data\n",
    "print(\"Shape of TF-IDF training data:\", X_train_tfidf.shape)\n",
    "print(\"Shape of TF-IDF testing data:\", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0dcda926-dd2c-4184-9497-3a15daa8fa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test set: ['female' 'female' 'male' 'female' 'female' 'male' 'female' 'female'\n",
      " 'male' 'male']\n"
     ]
    }
   ],
   "source": [
    "#3D. Build a base model for Supervised Learning - Classification.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Checking a few predictions\n",
    "print(\"Predictions on test set:\", y_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8389420-0afd-4a81-b831-9cdaf8ac5510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7159\n",
      "Precision: 0.7158\n",
      "Recall: 0.7159\n",
      "F1-Score: 0.7158\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      female       0.71      0.69      0.70      4585\n",
      "        male       0.72      0.74      0.73      4965\n",
      "\n",
      "    accuracy                           0.72      9550\n",
      "   macro avg       0.72      0.72      0.72      9550\n",
      "weighted avg       0.72      0.72      0.72      9550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3E. Clearly print Performance Metrics.\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e2680c6-6eb8-4079-bd74-b0cdf9b58690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer - Accuracy: 0.6977\n",
      "CountVectorizer - Precision: 0.6977\n",
      "CountVectorizer - Recall: 0.6977\n",
      "CountVectorizer - F1-Score: 0.6971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#4. Improve Performance of model. \n",
    "#4A. Experiment with other vectorisers. \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorize the text data using CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=5000)\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Build the Logistic Regression model again using CountVectorizer\n",
    "model_count = LogisticRegression()\n",
    "model_count.fit(X_train_count, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_count = model_count.predict(X_test_count)\n",
    "\n",
    "# Performance metrics for CountVectorizer\n",
    "accuracy_count = accuracy_score(y_test, y_pred_count)\n",
    "precision_count = precision_score(y_test, y_pred_count, average='weighted')\n",
    "recall_count = recall_score(y_test, y_pred_count, average='weighted')\n",
    "f1_count = f1_score(y_test, y_pred_count, average='weighted')\n",
    "\n",
    "# Print metrics for CountVectorizer\n",
    "print(f\"CountVectorizer - Accuracy: {accuracy_count:.4f}\")\n",
    "print(f\"CountVectorizer - Precision: {precision_count:.4f}\")\n",
    "print(f\"CountVectorizer - Recall: {recall_count:.4f}\")\n",
    "print(f\"CountVectorizer - F1-Score: {f1_count:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dec86402-13ca-4af5-8571-40fa22ccd157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Accuracy: 0.6766\n",
      "Random Forest - Precision: 0.6781\n",
      "Random Forest - Recall: 0.6766\n",
      "Random Forest - F1-Score: 0.6767\n"
     ]
    }
   ],
   "source": [
    "#4B. Build classifier Models using other algorithms than base model. \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Build a Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_tfidf, y_train)  # Using TF-IDF for this model\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# Performance metrics for Random Forest\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf, average='weighted')\n",
    "recall_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "\n",
    "# Print metrics for Random Forest\n",
    "print(f\"Random Forest - Accuracy: {accuracy_rf:.4f}\")\n",
    "print(f\"Random Forest - Precision: {precision_rf:.4f}\")\n",
    "print(f\"Random Forest - Recall: {recall_rf:.4f}\")\n",
    "print(f\"Random Forest - F1-Score: {f1_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b2f5012-12b4-4bd2-b77e-91717325f524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - Accuracy: 0.7151\n",
      "SVM - Precision: 0.7149\n",
      "SVM - Recall: 0.7151\n",
      "SVM - F1-Score: 0.7149\n"
     ]
    }
   ],
   "source": [
    "#4B. Build classifier Models using other algorithms than base model. \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Build an SVM model\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train_tfidf, y_train)  # Using TF-IDF for this model\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Performance metrics for SVM\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "precision_svm = precision_score(y_test, y_pred_svm, average='weighted')\n",
    "recall_svm = recall_score(y_test, y_pred_svm, average='weighted')\n",
    "f1_svm = f1_score(y_test, y_pred_svm, average='weighted')\n",
    "\n",
    "# Print metrics for SVM\n",
    "print(f\"SVM - Accuracy: {accuracy_svm:.4f}\")\n",
    "print(f\"SVM - Precision: {precision_svm:.4f}\")\n",
    "print(f\"SVM - Recall: {recall_svm:.4f}\")\n",
    "print(f\"SVM - F1-Score: {f1_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "942baef4-fa16-4c36-8cb1-af84e4055277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from GridSearchCV: {'C': 1, 'solver': 'lbfgs'}\n",
      "Best Logistic Regression - Accuracy: 0.7159\n",
      "Best Logistic Regression - Precision: 0.7158\n",
      "Best Logistic Regression - Recall: 0.7159\n",
      "Best Logistic Regression - F1-Score: 0.7158\n"
     ]
    }
   ],
   "source": [
    "#4C. Tune Parameters/Hyperparameters of the model/s. \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Tuning hyperparameters for Logistic Regression\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],   # Regularization parameter\n",
    "    'solver': ['liblinear', 'lbfgs']  # Solver to use\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(LogisticRegression(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the model on the training data\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "print(\"Best Parameters from GridSearchCV:\", grid_search.best_params_)\n",
    "\n",
    "# Predict with the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test_tfidf)\n",
    "\n",
    "# Performance metrics for the best model\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "precision_best = precision_score(y_test, y_pred_best, average='weighted')\n",
    "recall_best = recall_score(y_test, y_pred_best, average='weighted')\n",
    "f1_best = f1_score(y_test, y_pred_best, average='weighted')\n",
    "\n",
    "# Print metrics for the best model\n",
    "print(f\"Best Logistic Regression - Accuracy: {accuracy_best:.4f}\")\n",
    "print(f\"Best Logistic Regression - Precision: {precision_best:.4f}\")\n",
    "print(f\"Best Logistic Regression - Recall: {recall_best:.4f}\")\n",
    "print(f\"Best Logistic Regression - F1-Score: {f1_best:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b7e9531-f83a-404e-af02-12b22076cbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Performance Metrics:\n",
      "CountVectorizer - Accuracy: 0.6977, F1-Score: 0.6971\n",
      "Random Forest - Accuracy: 0.6766, F1-Score: 0.6767\n",
      "SVM - Accuracy: 0.7151, F1-Score: 0.7149\n",
      "Best Logistic Regression - Accuracy: 0.7159, F1-Score: 0.7158\n"
     ]
    }
   ],
   "source": [
    "#4D. Clearly print Performance Metrics.\n",
    "\n",
    "print(\"Comparison of Performance Metrics:\")\n",
    "\n",
    "# Print metrics for CountVectorizer Logistic Regression\n",
    "print(f\"CountVectorizer - Accuracy: {accuracy_count:.4f}, F1-Score: {f1_count:.4f}\")\n",
    "\n",
    "# Print metrics for Random Forest\n",
    "print(f\"Random Forest - Accuracy: {accuracy_rf:.4f}, F1-Score: {f1_rf:.4f}\")\n",
    "\n",
    "# Print metrics for SVM\n",
    "print(f\"SVM - Accuracy: {accuracy_svm:.4f}, F1-Score: {f1_svm:.4f}\")\n",
    "\n",
    "# Print metrics for Best Logistic Regression (with GridSearchCV)\n",
    "print(f\"Best Logistic Regression - Accuracy: {accuracy_best:.4f}, F1-Score: {f1_best:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3831af93-7680-423e-a792-a58830c4bd19",
   "metadata": {},
   "source": [
    "#### 5. Share insights on relative performance comparison.\n",
    "#### 5A. Which vectorizer performed better? Probable reason?\n",
    "\n",
    "The TF-IDF Vectorizer generally performed better than the Count Vectorizer across all models. This is perhaps because TF-IDF considers the frequency of terms within a document (like Count Vectorizer) as well as weighs down commonly occurring less informative words (e.g., stopwords) while giving more weight to rarer terms that are more informative. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f60627-ce57-41af-9d87-04fb0e1e70d2",
   "metadata": {},
   "source": [
    "#### 5B. Which model outperformed? Probable reason?\n",
    "\n",
    "The best-performing model is Logistic Regression (Accuracy: 0.7159, F1-Score: 0.7158), followed closely by SVM (Accuracy: 0.7151, F1-Score: 0.7149). A probable reason is that Logistic Regression, being a simple linear model, often works well for high-dimensional data like text data when combined with appropriate feature engineering (vectorization in this case). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c7632-40e1-49b8-83a8-5daf99518287",
   "metadata": {},
   "source": [
    "#### 5C. Which parameter/hyperparameter significantly helped to improve performance?Probable reason?\n",
    "\n",
    "Regularization helps to prevent overfitting by penalizing large coefficients, which is especially important when dealing with sparse, high-dimensional data like text. In SVM, kernel choices or tuning of the C parameter might have also improved performance by controlling the trade-off between achieving a higher margin and allowing some misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ab491-14bc-4086-a167-d7bef0c62fb3",
   "metadata": {},
   "source": [
    "#### 5D. According to you, which performance metric should be given most importance, why?\n",
    "\n",
    "In this case, the F1-Score should be given the most importance. This is because F1-Score balances precision and recall, making it more informative when dealing with imbalanced datasets or multi-class problems, which are common in NLP tasks. Accuracy alone can be misleading if there’s a class imbalance or if false positives/negatives have different costs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
