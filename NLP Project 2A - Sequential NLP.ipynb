{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64be85a-a9e6-4e68-93a1-403444f1cffd",
   "metadata": {},
   "source": [
    "# Part A"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f790426-7af9-4ea7-b505-cdc123d7e32f",
   "metadata": {},
   "source": [
    "• DOMAIN: Digital content and entertainment industry\n",
    "• CONTEXT: The objective of this project is to build a text classification model that analyses the customer's\n",
    "sentiments based on their reviews in the IMDB database. The model uses a complex deep learning model to build\n",
    "an embedding layer followed by a classification algorithm to analyse the sentiment of the customers.\n",
    "• DATA DESCRIPTION: The Dataset of 50,000 movie reviews from IMDB, labelled by sentiment (positive/negative).\n",
    "Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For\n",
    "convenience, the words are indexed by their frequency in the dataset, meaning the for that has index 1 is the most\n",
    "frequent word. Use the first 20 words from each review to speed up training, using a max vocabulary size of\n",
    "10,000. As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
    "• PROJECT OBJECTIVE: To Build a sequential NLP classifier which can use input text parameters to determine the\n",
    "customer sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3206a2f-e65b-4dd4-822a-ef5124efa5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (25000,)\n",
      "Training labels shape: (25000,)\n",
      "Test data shape: (25000,)\n",
      "Test labels shape: (25000,)\n",
      "Sample review (encoded): [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "Sample label: 1\n"
     ]
    }
   ],
   "source": [
    "#1. Import and analyse the data set.\n",
    "\n",
    "# Import required libraries and load the dataset\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# Load the dataset and keep the 10,000 most frequent words\n",
    "vocabulary_size = 10000\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocabulary_size)\n",
    "\n",
    "# Check the shape of the data\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Training labels shape: {train_labels.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")\n",
    "\n",
    "# Print a sample from the dataset\n",
    "print(f\"Sample review (encoded): {train_data[0]}\")\n",
    "print(f\"Sample label: {train_labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25d2966d-9926-4bd5-8628-926da3fd2699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded training data shape: (25000, 20)\n",
      "Padded test data shape: (25000, 20)\n",
      "Sample review (padded): [  65   16   38 1334   88   12   16  283    5   16 4472  113  103   32\n",
      "   15   16 5345   19  178   32]\n"
     ]
    }
   ],
   "source": [
    "# 2. Perform relevant sequence adding on the data.\n",
    "# Perform sequence padding to make all reviews the same length\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad sequences to ensure each review is exactly 20 words long\n",
    "max_length = 20\n",
    "train_data_padded = pad_sequences(train_data, maxlen=max_length)\n",
    "test_data_padded = pad_sequences(test_data, maxlen=max_length)\n",
    "\n",
    "# Check the shape after padding\n",
    "print(f\"Padded training data shape: {train_data_padded.shape}\")\n",
    "print(f\"Padded test data shape: {test_data_padded.shape}\")\n",
    "\n",
    "# Print a sample after padding\n",
    "print(f\"Sample review (padded): {train_data_padded[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3acc911-88a6-41c7-9541-ccaa20305d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded training data shape: (25000, 20)\n",
      "Padded training labels shape: (25000,)\n",
      "Sample review (padded): [  65   16   38 1334   88   12   16  283    5   16 4472  113  103   32\n",
      "   15   16 5345   19  178   32]\n",
      "Corresponding label: 1\n"
     ]
    }
   ],
   "source": [
    "#3. Perform following data analysis.\n",
    "#3A. Print shape of features and labels\n",
    "#3B. Print value of any one feature and it's label\n",
    "\n",
    "# Print the shape of the padded features (reviews) and labels (sentiments)\n",
    "print(f\"Padded training data shape: {train_data_padded.shape}\")\n",
    "print(f\"Padded training labels shape: {train_labels.shape}\")\n",
    "\n",
    "# Print one sample feature (padded review) and its corresponding label\n",
    "sample_index = 0  # You can change this index to see different samples\n",
    "print(f\"Sample review (padded): {train_data_padded[sample_index]}\")\n",
    "print(f\"Corresponding label: {train_labels[sample_index]}\")  # 0 for negative, 1 for positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00ab45af-b806-4beb-be8e-f13740f6f543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded review: story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "#4. Decode the feature value to get original sentence\n",
    "\n",
    "# Get the word index from the IMDB dataset\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# The word index is 1-based, but we need it to be 0-based for decoding\n",
    "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
    "\n",
    "# The IMDB dataset adds a few reserved indices\n",
    "reverse_word_index = {k+3: v for k, v in reverse_word_index.items()}\n",
    "reverse_word_index[0] = \"<PAD>\"  # Padding\n",
    "reverse_word_index[1] = \"<START>\"  # Start of a review\n",
    "reverse_word_index[2] = \"<UNK>\"  # Unknown word\n",
    "reverse_word_index[3] = \"<UNUSED>\"  # Unused word\n",
    "\n",
    "# Decode a sample review from padded data back to words\n",
    "def decode_review(encoded_review):\n",
    "    return ' '.join([reverse_word_index.get(i, \"?\") for i in encoded_review])\n",
    "\n",
    "# Choose any sample review to decode (for example, the first one)\n",
    "sample_review = train_data_padded[0]\n",
    "print(f\"Decoded review: {decode_review(sample_review)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6845e2f8-b7c2-44db-b9af-0f00773b6ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.5840 - loss: 0.6860 - val_accuracy: 0.6766 - val_loss: 0.5953\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.7225 - loss: 0.5473 - val_accuracy: 0.7376 - val_loss: 0.5191\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.8038 - loss: 0.4275 - val_accuracy: 0.7502 - val_loss: 0.4974\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.8402 - loss: 0.3670 - val_accuracy: 0.7472 - val_loss: 0.5193\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.8631 - loss: 0.3264 - val_accuracy: 0.7454 - val_loss: 0.5387\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.8834 - loss: 0.2950 - val_accuracy: 0.7394 - val_loss: 0.5825\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9002 - loss: 0.2646 - val_accuracy: 0.7340 - val_loss: 0.5978\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9108 - loss: 0.2422 - val_accuracy: 0.7198 - val_loss: 0.7189\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9190 - loss: 0.2151 - val_accuracy: 0.7292 - val_loss: 0.6813\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9359 - loss: 0.1919 - val_accuracy: 0.7228 - val_loss: 0.7788\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7287 - loss: 0.7628\n",
      "Test Accuracy: 0.7279599905014038\n"
     ]
    }
   ],
   "source": [
    "#5. Design, train, tune and test a sequential model.\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, LSTM\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an embedding layer (vocabulary size is 10,000 and output dimension is 32)\n",
    "model.add(Embedding(input_dim=vocabulary_size, output_dim=32, input_length=max_length))\n",
    "\n",
    "# Add an LSTM layer\n",
    "model.add(LSTM(32))\n",
    "\n",
    "# Add a dense layer with 1 output (for binary classification)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_data_padded, train_labels, epochs=10, batch_size=512, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(test_data_padded, test_labels)\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a058639-862a-43cd-8287-8ecc73a8955b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494ms/step\n",
      "Predicted sentiment: negative\n",
      "Actual sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "#6. Use the designed model to print the prediction on any one sample.\n",
    "\n",
    "# Select a sample review from the test data (for example, the first test review)\n",
    "sample_review = test_data_padded[5]\n",
    "\n",
    "# Predict the sentiment\n",
    "prediction = model.predict(np.array([sample_review]))\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted sentiment: {'positive' if prediction[0] > 0.5 else 'negative'}\")\n",
    "print(f\"Actual sentiment: {'positive' if test_labels[0] == 1 else 'negative'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b858f1a-b7e3-4ac6-8888-dda4bc02a467",
   "metadata": {},
   "source": [
    "### Further experimentations to improve the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c569cac3-dea8-4d46-9d9e-3cd84dde1ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 55ms/step - accuracy: 0.6004 - loss: 0.6579 - val_accuracy: 0.7362 - val_loss: 0.5206\n",
      "Epoch 2/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.8040 - loss: 0.4292 - val_accuracy: 0.7486 - val_loss: 0.4940\n",
      "Epoch 3/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.8541 - loss: 0.3511 - val_accuracy: 0.7460 - val_loss: 0.5336\n",
      "Epoch 4/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.8830 - loss: 0.2990 - val_accuracy: 0.7376 - val_loss: 0.5606\n",
      "Epoch 5/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.9033 - loss: 0.2434 - val_accuracy: 0.7332 - val_loss: 0.6169\n",
      "Epoch 6/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9294 - loss: 0.1968 - val_accuracy: 0.7316 - val_loss: 0.6691\n",
      "Epoch 7/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9433 - loss: 0.1649 - val_accuracy: 0.7166 - val_loss: 0.8393\n",
      "Epoch 8/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.9515 - loss: 0.1354 - val_accuracy: 0.7130 - val_loss: 0.8976\n",
      "Epoch 9/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9601 - loss: 0.1123 - val_accuracy: 0.7048 - val_loss: 1.0779\n",
      "Epoch 10/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.9683 - loss: 0.0950 - val_accuracy: 0.7080 - val_loss: 1.2464\n",
      "Epoch 11/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9696 - loss: 0.0866 - val_accuracy: 0.7020 - val_loss: 1.2558\n",
      "Epoch 12/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.9817 - loss: 0.0577 - val_accuracy: 0.7034 - val_loss: 1.5543\n",
      "Epoch 13/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.9835 - loss: 0.0508 - val_accuracy: 0.6958 - val_loss: 1.5201\n",
      "Epoch 14/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.9873 - loss: 0.0412 - val_accuracy: 0.6974 - val_loss: 1.6172\n",
      "Epoch 15/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.9866 - loss: 0.0411 - val_accuracy: 0.6942 - val_loss: 1.8067\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7094 - loss: 1.6938\n",
      "Test Accuracy: 0.7049599885940552\n"
     ]
    }
   ],
   "source": [
    "# Increase the LSTM units from 32 to 64 for better learning capacity.\n",
    "# Add a Dropout layer to prevent overfitting with a 50% dropout rate.\n",
    "# Increase the batch size to 256 and train for 15 epochs for better generalization.\n",
    "                                                        \n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Step: Experimenting with larger LSTM units and adding Dropout\n",
    "\n",
    "# Define the model with modifications\n",
    "model = Sequential()\n",
    "\n",
    "# Add an embedding layer\n",
    "model.add(Embedding(input_dim=vocabulary_size, output_dim=64, input_length=max_length))\n",
    "\n",
    "# Add an LSTM layer with more units and dropout\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add a dense layer with sigmoid for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with the same optimizer and loss function\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model again with new parameters\n",
    "history = model.fit(train_data_padded, train_labels, epochs=15, batch_size=256, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model again\n",
    "test_loss, test_acc = model.evaluate(test_data_padded, test_labels)\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
